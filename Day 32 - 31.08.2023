Apache Spark - core technology for large-scale data analytics. Learn how to use Spark in ASA to analyse and visualise data in data lake.

It is important to remember that Apache Spark is a open source parallel processing framework for large-scale data processing and analytics. It has become 
extremely popular in big data processing scenarios, and is available in multiple platform implementations.

This includes:

Azure HDInishgt
Azure Datatricks
Azure Synapse Analytics

The purpose of this module is to use Spark in ASA to ingest, process and analyse data from a data lake. 

1) Identify the core features and capabilities of Apache Spark.
2) Configure a Spark pool in Azure Synapse Analytics
3) Run code to load, analyse and visualise data in a Spark notebook.

https://learn.microsoft.com/en-gb/training/modules/understand-big-data-engineering-with-apache-spark-azure-synapse-analytics/2-get-to-know-spark

https://learn.microsoft.com/en-gb/training/modules/understand-big-data-engineering-with-apache-spark-azure-synapse-analytics/3-use-spark


Step 1 ) Loading data into a dataframe using Python

%%pyspark

def = spark.read.load('fileurl/file.csv',
    format='csv',
    header=True
)
display(df.limit(10))

Step 2 ) Specifying a dataframe schema

from pyspark.sql.types import *
from pyspark.sql.functions import *

productSchema = StructType([
        StructField("ProductID", IntegerType()),
        StructField("ProductName", StringType()),
        StructField("Category", StringType()),
        StructField("ListPrice", FloatType())
        ])

df = spark.read.load('fileurl/file.csv',
    format='csv',
    header=False)
display(df.limit(10))


Step 3 )   Filtering and Grouping Dataframes

pricelist_df = df["ProductID", "ListPrice"]



Step 4 )   Finding with a WHERE clause in dataframe

bikes_df = df.select("ProductName", "ListPrice"). where((df["Category"]=="Mountain Bikes") | df["Category"]=="Road Bikes"))

display(bikes_df)



