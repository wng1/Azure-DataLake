https://learn.microsoft.com/en-gb/training/modules/transform-data-spark-azure-synapse-analytics/3-partition-data

The Parquet format is typically preferred for data files that you will use for further analysis or ingestion into an analytical store.
Parquet is a very efficient format that is supported by most large scale data analytics systems. 
In fact, sometimes your data transformation requirement may simply be to convert data from another format (such as CSV) to Parquet!


from pyspark.sql.functions import year, col

df = spark.read.csv('________________', header=True, inferSchema=True)
dated_df = df.withColumn("Year", year(col("OrderDate")))

# The folder name is generated when partioning a dataframe include the partitioning column name and value in column=value format, as shown here:

dated_df.write.partitionBy("Year").mode("overwrite").parquest("/data")



#Filter parquet files in a query

When reading data from parquet files into a dataframe, you have the ability to pull data from any folder within the hierarchical folders. 
This filtering process is done with the use of explicit values and wildcards against the partitioned fields.

In the following example, the following code will pull the sales orders, which were placed in 2020.

orders_2020 = spark.read.parquet('_____________')
display(order_2020.limit(5))




#Transform data with SQL

https://learn.microsoft.com/en-gb/training/modules/transform-data-spark-azure-synapse-analytics/4-tramsform-sql


THe SparkSQL Library, which provides the dataframe structure also enables you to use SQL as a way of working with data. 
With this approahc, you can "query" and "transform" data in dataframes by using SQL queries, and "persist" the results as tables.

External Tables are "loosely bound" to the underlyign files and deleting the table does not delete the files.
This allows you to use Spark to do the heavy lifting of transformation then persist the data in the lake. After this is done you can
drop the table and downstream processes can access these optimized structures.


#Save loaded csv file as an externtable name (sales_order) and store in data lake as /sales_orders_table
order_details.write.saveAsTable('sale_orders', format='parquet', mode='overwrite', path='/sales_orders_table')


#Use SQL to query and transform the data

#Python
#Create derived columns

sql_tranform = spark.sql("SELECT *, YEAR(OrderDate) AS Year, MONTH(OrderDate) AS Month FROM sales_orders")

#Save the results
sql_transform.write.partitionBy("Year","Month").saveAsTable('transformed_orders', format='parquet', mode#='overwrite', path='/transformed_orders_table')



#Query the metastore

%%sql

SELECT * FROM transformed_orders
WHERE Year = 2021
  AND Month = 1

#Drop the tables
DROP TABLE transformed_orders;
DROP TABLE sales_orders;


##Use Delta Lake in Azure Synapse Analytics

-- Describe core features and capabilities of Delta Lake
-- Create and use Delta Lake table in a Synapse Analytics Spark pool
-- Create Spark catalog tables for Delta Lake data
-- Use Delta Lake tables for streaming data
-- Query Delta Lake tables from a Synapse Analytics SQL pool

Linux foundation (Delta Lake) is an open-source storage layer for Spark that enables relational database capabilities for batch and streaming data.

By using Delta Lake, you can implement a data lakehouse architecture in Spark to support SQL base data manipulation semantics with support for transactions and schema
enforcement.

- Support for batch and streaming data. While most relational databases include tables that store static data, Spark includes native support for streaming data through the Spark Structured Streaming API. Delta Lake tables can be used as both sinks (destinations) and sources for streaming data.
- Standard formats and interoperability. The underlying data for Delta Lake tables is stored in Parquet format, which is commonly used in data lake ingestion pipelines. Additionally, you can use the serverless SQL pool in Azure Synapse Analytics to query Delta Lake tables in SQL.

--https://learn.microsoft.com/en-gb/training/modules/use-delta-lake-azure-synapse-analytics/3-create-delta-tables
-- Python
#Load a file into a dataframe

df = spark.read.load('.../test.csv', format='csv', header=True)

#Save the dataframe as a delta table
delta_table_path = "/folderpath"
df.write.format("delta").save(delta_table_path)

#You can replace an existing Delta Lake table with the contents of a dataframe by using the overwrite mode, as shown here:

new_df.write.format("delta").mode("overwrite").save(delta_table_path)

#You can also add rows from a dataframe to an existing table by using the append mode:

new_append_df.write.format("delta").mode("append").save(delta_table_path)


#DeltaLake API has a DeltaTable object that can be used to support in the update, delete, merge operations. The following code can be used 
#to update the price column for all rows with a category column value of "Accessories".


-- Python

from delta.tables import *
from pyspark.sql.functions import *

#Create a DeltaTable object
deltaTable = DeltaTable.forPath(spark, delta_table_path)

#Update the table (reduce price of accessories by 10%)
deltaTable.update(
  condition = "Category == 'Accessories'",
  set = { "Price": "Price * 0.9"})


##Querying a previous version of a table

df = spark.read.format("delta").option("versionAsOf", 0).load(delta_table_path)
df = spark.read.format("delta").option("timestampAsOf", '2022-01-01').load(delta_table_path)

##Creating catalog tables

# Save a dataframe as a managed table
df.write.format("delta").saveAsTable("MyManagedTable")

## specify a path option to save as an external table
df.write.format("delta").option("path", "/mydata").saveAsTable("MyExternalTable")

##Creating a catalog table using SQL via Python
spark.sql("CREATE TABLE MyExternalTable USING DELTA LOCATION '/mydata'")

##OR via SQL 

%%sql

CREATE TABLE MyExternalTable
USING DELTA
LOCATION '/mydata'

##Defining the table schema

%%sql

CREATE TABLE ManagedSalesOrders
(
    Orderid INT NOT NULL,
    OrderDate TIMESTAMP NOT NULL,
    CustomerName STRING,
    SalesTotal FLOAT NOT NULL
)
USING DELTA


Using the DeltaTableBuilder API

from delta.tables import *

DeltaTable.create(spark) \
  .tableName("default.ManagedProducts") \
  .addColumn("Productid", "INT") \
  .addColumn("ProductName", "STRING") \
  .addColumn("Category", "STRING") \
  .addColumn("Price", "FLOAT") \
  .execute()


%%sql

SELECT orderid, salestotal
FROM ManagedSalesOrders



##Querying delta formatted files with OPENROWSET

SELECT *
FROM
    OPENROWSET(
        BULK 'https://..../delta/mytable/',
        FORMAT = 'DELTA'
    ) AS deltadata



##Create database and add location of data source by OPENROWSET

CREATE DATABASE MyDB
      COLLATE Latin1_General_100_BIN2_UTF8;
GO;

USE MyDB;
GO

CREATE EXTERNAL DATA SOURCE DeltaLakeStore
WITH
(
    LOCATION = 'https://mystore.dfs.core.windows.net/files/delta/'
);
GO

SELECT TOP 10 *
FROM OPENROWSET(
        BULK 'mytable',
        DATA_SOURCE = 'DeltaLakeStore',
        FORMAT = 'DELTA'
    ) as deltadata;


##Querying Catalog Tables

USE default;

SELECT * FROM MyDeltaTable;


